CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python
pip install 'llama-cpp-python[server]'
pip install torch
