python -m llama_cpp.server --n_ctx=32768 --n_gpu_layers=33 --model ./mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf --chat_format chatml
